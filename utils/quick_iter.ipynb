{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79907e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnwu/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/onnx/_internal/_beartype.py:36: UserWarning: unhashable type: 'list'\n",
      "  warnings.warn(f\"{e}\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    \n",
    "from list_prompt_family import ListPromptFamily\n",
    "from prompt_registry import PROMPT_REGISTRY, ALL_PROMPTS\n",
    "from wrap_registry import WRAP_REGISTRY\n",
    "import numpy as np\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformer_lens import HookedTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3261b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "The token `transformerlens` has been saved to /Users/johnwu/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /Users/johnwu/.cache/huggingface/token\n",
      "Login successful.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "!huggingface-cli login --token {hf_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b09d428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading model microsoft/phi-2 requires setting trust_remote_code=True\n",
      "WARNING:root:Loading model microsoft/phi-2 state dict requires setting trust_remote_code=True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3530b15202984760a55201f0ec8da720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Phi-2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"Phi-2\"\n",
    "torch.mps.empty_cache()\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device=\"mps\",  \n",
    ")           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c98f122e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clean': 'What is the output of the following python code [2, 1, 1, 1, 1].index(2)?\\nAnswer:', 'corrupted': 'What is the output of the following python code [1, 2, 1, 1, 1].index(2)?\\nAnswer:'}\n",
      "{'clean': 15, 'corrupted': 16}\n",
      "{'clean': 'What is the output of the following python code [2, 1, 1, 1, 1].index(2)?\\nAnswer:', 'corrupted': 'What is the output of the following python code [1, 1, 2, 1, 1].index(2)?\\nAnswer:'}\n",
      "{'clean': 15, 'corrupted': 17}\n",
      "{'clean': 'What is the output of the following python code [2, 1, 1, 1, 1].index(2)?\\nAnswer:', 'corrupted': 'What is the output of the following python code [1, 1, 1, 2, 1].index(2)?\\nAnswer:'}\n",
      "{'clean': 15, 'corrupted': 18}\n",
      "{'clean': 'What is the output of the following python code [2, 1, 1, 1, 1].index(2)?\\nAnswer:', 'corrupted': 'What is the output of the following python code [1, 1, 1, 1, 2].index(2)?\\nAnswer:'}\n",
      "{'clean': 15, 'corrupted': 19}\n",
      "{'clean': 'What is the output of the following python code [1, 2, 1, 1, 1].index(2)?\\nAnswer:', 'corrupted': 'What is the output of the following python code [2, 1, 1, 1, 1].index(2)?\\nAnswer:'}\n",
      "{'clean': 16, 'corrupted': 15}\n",
      "{'clean': 'What is the output of the following python code [1, 2, 1, 1, 1].index(2)?\\nAnswer:', 'corrupted': 'What is the output of the following python code [1, 1, 2, 1, 1].index(2)?\\nAnswer:'}\n",
      "{'clean': 16, 'corrupted': 17}\n",
      "{'clean': 'What is the output of the following python code [1, 2, 1, 1, 1].index(2)?\\nAnswer:', 'corrupted': 'What is the output of the following python code [1, 1, 1, 2, 1].index(2)?\\nAnswer:'}\n",
      "{'clean': 16, 'corrupted': 18}\n",
      "{'clean': 'What is the output of the following python code [1, 2, 1, 1, 1].index(2)?\\nAnswer:', 'corrupted': 'What is the output of the following python code [1, 1, 1, 1, 2].index(2)?\\nAnswer:'}\n",
      "{'clean': 16, 'corrupted': 19}\n",
      "{'clean': 'What is the output of the following python code [1, 1, 2, 1, 1].index(2)?\\nAnswer:', 'corrupted': 'What is the output of the following python code [2, 1, 1, 1, 1].index(2)?\\nAnswer:'}\n",
      "{'clean': 17, 'corrupted': 15}\n",
      "{'clean': 'What is the output of the following python code [1, 1, 2, 1, 1].index(2)?\\nAnswer:', 'corrupted': 'What is the output of the following python code [1, 2, 1, 1, 1].index(2)?\\nAnswer:'}\n",
      "{'clean': 17, 'corrupted': 16}\n",
      "{'clean': 'What is the output of the following python code [1, 1, 2, 1, 1].index(2)?\\nAnswer:', 'corrupted': 'What is the output of the following python code [1, 1, 1, 2, 1].index(2)?\\nAnswer:'}\n",
      "{'clean': 17, 'corrupted': 18}\n",
      "{'clean': 'What is the output of the following python code [1, 1, 2, 1, 1].index(2)?\\nAnswer:', 'corrupted': 'What is the output of the following python code [1, 1, 1, 1, 2].index(2)?\\nAnswer:'}\n",
      "{'clean': 17, 'corrupted': 19}\n",
      "{'clean': 'What is the output of the following python code [1, 1, 1, 2, 1].index(2)?\\nAnswer:', 'corrupted': 'What is the output of the following python code [2, 1, 1, 1, 1].index(2)?\\nAnswer:'}\n",
      "{'clean': 18, 'corrupted': 15}\n",
      "{'clean': 'What is the output of the following python code [1, 1, 1, 2, 1].index(2)?\\nAnswer:', 'corrupted': 'What is the output of the following python code [1, 2, 1, 1, 1].index(2)?\\nAnswer:'}\n",
      "{'clean': 18, 'corrupted': 16}\n",
      "{'clean': 'What is the output of the following python code [1, 1, 1, 2, 1].index(2)?\\nAnswer:', 'corrupted': 'What is the output of the following python code [1, 1, 2, 1, 1].index(2)?\\nAnswer:'}\n",
      "{'clean': 18, 'corrupted': 17}\n",
      "{'clean': 'What is the output of the following python code [1, 1, 1, 2, 1].index(2)?\\nAnswer:', 'corrupted': 'What is the output of the following python code [1, 1, 1, 1, 2].index(2)?\\nAnswer:'}\n",
      "{'clean': 18, 'corrupted': 19}\n",
      "{'clean': 'What is the output of the following python code [1, 1, 1, 1, 2].index(2)?\\nAnswer:', 'corrupted': 'What is the output of the following python code [2, 1, 1, 1, 1].index(2)?\\nAnswer:'}\n",
      "{'clean': 19, 'corrupted': 15}\n",
      "{'clean': 'What is the output of the following python code [1, 1, 1, 1, 2].index(2)?\\nAnswer:', 'corrupted': 'What is the output of the following python code [1, 2, 1, 1, 1].index(2)?\\nAnswer:'}\n",
      "{'clean': 19, 'corrupted': 16}\n",
      "{'clean': 'What is the output of the following python code [1, 1, 1, 1, 2].index(2)?\\nAnswer:', 'corrupted': 'What is the output of the following python code [1, 1, 2, 1, 1].index(2)?\\nAnswer:'}\n",
      "{'clean': 19, 'corrupted': 17}\n",
      "{'clean': 'What is the output of the following python code [1, 1, 1, 1, 2].index(2)?\\nAnswer:', 'corrupted': 'What is the output of the following python code [1, 1, 1, 2, 1].index(2)?\\nAnswer:'}\n",
      "{'clean': 19, 'corrupted': 18}\n"
     ]
    }
   ],
   "source": [
    "def gen_list_comprehension_prompt(start: int, end: int, step: int):\n",
    "    return f'What is the output of [i for i in range({start}, {end}, {step})]. Only output a list, no other information. List:', [f'{i}' for i in range(start, end, step)]\n",
    "\n",
    "def gen_find_index_prompt(elm, correct):\n",
    "    return lambda lst: f\"What is the output of the following python code {lst}.index({elm})?\\nAnswer:\", correct\n",
    "\n",
    "prompts = []\n",
    "\n",
    "lsts = []\n",
    "for i in range(5): \n",
    "    lst = [1, 1, 1, 1, 1]\n",
    "    lst[i] = 2\n",
    "    lsts.append(lst)\n",
    "    prompt_fn, correct = gen_find_index_prompt(2, i)\n",
    "    prompt = prompt_fn(lst)\n",
    "    prompts.append(prompt_fn)\n",
    "    # answer = model.generate(prompt, temperature=0.0, top_k=0, max_new_tokens=1)\n",
    "    # print(model.to_str_tokens(answer))\n",
    "    \n",
    "pairs = []\n",
    "answers = []\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if i != j:\n",
    "            pairs.append({'clean': prompts[i](lsts[i]), 'corrupted': prompts[i](lsts[j])})\n",
    "            answers.append({'clean': model.to_single_token(f\"{i}\"), 'corrupted': model.to_single_token(f\"{j}\")})\n",
    "for pair, answer in zip(pairs, answers): \n",
    "    print(pair)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d6e523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = model.to_tokens([pair['clean'] for pair in pairs])\n",
    "corrupted_tokens = model.to_tokens([pair['corrupted'] for pair in pairs])\n",
    "\n",
    "answer_token_indices = torch.tensor([[answers[i]['clean'], answers[i]['corrupted']] for i in range(len(answers))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "912dbc72",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 27.21 GB, other allocations: 672.00 KB, max allowed: 27.20 GB). Tried to allocate 5.47 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     incorrect_logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, answer_token_indices[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (correct_logits \u001b[38;5;241m-\u001b[39m incorrect_logits)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m----> 8\u001b[0m clean_logits, clean_cache \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mcurrent_allocated_memory())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/transformer_lens/HookedTransformer.py:694\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_cache\u001b[39m(\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mmodel_args, return_cache_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    679\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    686\u001b[0m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m    687\u001b[0m ]:\n\u001b[1;32m    688\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;124;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m     out, cache_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    698\u001b[0m         cache \u001b[38;5;241m=\u001b[39m ActivationCache(cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/transformer_lens/hook_points.py:569\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, pos_slice, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m cache_dict, fwd, bwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    556\u001b[0m     names_filter,\n\u001b[1;32m    557\u001b[0m     incl_bwd,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    560\u001b[0m     pos_slice\u001b[38;5;241m=\u001b[39mpos_slice,\n\u001b[1;32m    561\u001b[0m )\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[1;32m    564\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39mfwd,\n\u001b[1;32m    565\u001b[0m     bwd_hooks\u001b[38;5;241m=\u001b[39mbwd,\n\u001b[1;32m    566\u001b[0m     reset_hooks_end\u001b[38;5;241m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    567\u001b[0m     clear_contexts\u001b[38;5;241m=\u001b[39mclear_contexts,\n\u001b[1;32m    568\u001b[0m ):\n\u001b[0;32m--> 569\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    571\u001b[0m         model_out\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/transformer_lens/HookedTransformer.py:583\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mLocallyOverridenDefaults(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m, prepend_bos\u001b[38;5;241m=\u001b[39mprepend_bos, padding_side\u001b[38;5;241m=\u001b[39mpadding_side\n\u001b[1;32m    576\u001b[0m ):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m         (\n\u001b[1;32m    579\u001b[0m             residual,\n\u001b[1;32m    580\u001b[0m             tokens,\n\u001b[1;32m    581\u001b[0m             shortformer_pos_embed,\n\u001b[1;32m    582\u001b[0m             attention_mask,\n\u001b[0;32m--> 583\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_to_embed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/transformer_lens/HookedTransformer.py:409\u001b[0m, in \u001b[0;36mHookedTransformer.input_to_embed\u001b[0;34m(self, input, prepend_bos, padding_side, attention_mask, past_kv_cache)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_hook_tokens:\n\u001b[1;32m    407\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_tokens(tokens)\n\u001b[0;32m--> 409\u001b[0m embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_embed(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    410\u001b[0m residual, shortformer_pos_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_residual(\n\u001b[1;32m    411\u001b[0m     embed,\n\u001b[1;32m    412\u001b[0m     pos_offset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m     return_shortformer_pos_embed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    417\u001b[0m )\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m residual, tokens, shortformer_pos_embed, attention_mask\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/transformer_lens/components/embed.py:34\u001b[0m, in \u001b[0;36mEmbed.forward\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mpost_embedding_ln:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_E[tokens, :])\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_E\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 27.21 GB, other allocations: 672.00 KB, max allowed: 27.20 GB). Tried to allocate 5.47 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "def get_logit_diff(logits, answer_token_indices=answer_token_indices):\n",
    "    if len(logits.shape)==3:\n",
    "        logits = logits[:, -1, :]\n",
    "    correct_logits = logits.gather(1, answer_token_indices[:, 0].unsqueeze(1))\n",
    "    incorrect_logits = logits.gather(1, answer_token_indices[:, 1].unsqueeze(1))\n",
    "    return (correct_logits - incorrect_logits).mean()\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "torch.mps.empty_cache()\n",
    "print(torch.mps.current_allocated_memory())\n",
    "# corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "\n",
    "# clean_logit_diff = get_logit_diff(clean_logits, answer_token_indices).item()\n",
    "# print(f\"Clean logit diff: {clean_logit_diff:.4f}\")\n",
    "\n",
    "# corrupted_logit_diff = get_logit_diff(corrupted_logits, answer_token_indices).item()\n",
    "# print(f\"Corrupted logit diff: {corrupted_logit_diff:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9069d9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 7.9192, 11.4271, 13.7004,  ..., -3.7537, -3.7544, -3.7543],\n",
      "         [ 6.8309,  4.9825,  1.6050,  ..., -3.3869, -3.3868, -3.3861],\n",
      "         [ 2.8343,  2.0546, -0.1482,  ..., -3.0613, -3.0606, -3.0603],\n",
      "         ...,\n",
      "         [ 6.5264, 12.5410, 14.7584,  ..., -0.6470, -0.6481, -0.6498],\n",
      "         [ 8.8152,  7.9323,  3.3557,  ..., -1.3492, -1.3488, -1.3499],\n",
      "         [ 4.6517,  6.4557,  6.7365,  ..., -2.2235, -2.2232, -2.2245]],\n",
      "\n",
      "        [[ 7.9192, 11.4271, 13.7004,  ..., -3.7537, -3.7544, -3.7543],\n",
      "         [ 6.8309,  4.9825,  1.6050,  ..., -3.3869, -3.3868, -3.3861],\n",
      "         [ 2.8343,  2.0546, -0.1482,  ..., -3.0613, -3.0606, -3.0603],\n",
      "         ...,\n",
      "         [ 6.5264, 12.5410, 14.7584,  ..., -0.6470, -0.6481, -0.6498],\n",
      "         [ 8.8152,  7.9323,  3.3557,  ..., -1.3492, -1.3488, -1.3499],\n",
      "         [ 4.6517,  6.4557,  6.7365,  ..., -2.2235, -2.2232, -2.2245]],\n",
      "\n",
      "        [[ 7.9192, 11.4271, 13.7004,  ..., -3.7537, -3.7544, -3.7543],\n",
      "         [ 6.8309,  4.9825,  1.6050,  ..., -3.3869, -3.3868, -3.3861],\n",
      "         [ 2.8343,  2.0546, -0.1482,  ..., -3.0613, -3.0606, -3.0603],\n",
      "         ...,\n",
      "         [ 6.5264, 12.5410, 14.7584,  ..., -0.6470, -0.6481, -0.6498],\n",
      "         [ 8.8152,  7.9323,  3.3557,  ..., -1.3492, -1.3488, -1.3499],\n",
      "         [ 4.6517,  6.4557,  6.7365,  ..., -2.2235, -2.2232, -2.2245]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 7.9192, 11.4271, 13.7004,  ..., -3.7537, -3.7544, -3.7543],\n",
      "         [ 6.8309,  4.9825,  1.6050,  ..., -3.3869, -3.3868, -3.3861],\n",
      "         [ 2.8343,  2.0546, -0.1482,  ..., -3.0613, -3.0606, -3.0603],\n",
      "         ...,\n",
      "         [ 6.4815, 12.7714, 14.6540,  ..., -0.6680, -0.6692, -0.6711],\n",
      "         [ 8.5918,  8.0394,  3.2524,  ..., -1.2422, -1.2418, -1.2429],\n",
      "         [ 5.2291,  5.9410,  5.4271,  ..., -1.9422, -1.9423, -1.9442]],\n",
      "\n",
      "        [[ 7.9192, 11.4271, 13.7004,  ..., -3.7537, -3.7544, -3.7543],\n",
      "         [ 6.8309,  4.9825,  1.6050,  ..., -3.3869, -3.3868, -3.3861],\n",
      "         [ 2.8343,  2.0546, -0.1482,  ..., -3.0613, -3.0606, -3.0603],\n",
      "         ...,\n",
      "         [ 6.4815, 12.7714, 14.6540,  ..., -0.6680, -0.6692, -0.6711],\n",
      "         [ 8.5918,  8.0394,  3.2524,  ..., -1.2422, -1.2418, -1.2429],\n",
      "         [ 5.2291,  5.9410,  5.4271,  ..., -1.9422, -1.9423, -1.9442]],\n",
      "\n",
      "        [[ 7.9192, 11.4271, 13.7004,  ..., -3.7537, -3.7544, -3.7543],\n",
      "         [ 6.8309,  4.9825,  1.6050,  ..., -3.3869, -3.3868, -3.3861],\n",
      "         [ 2.8343,  2.0546, -0.1482,  ..., -3.0613, -3.0606, -3.0603],\n",
      "         ...,\n",
      "         [ 6.4815, 12.7714, 14.6540,  ..., -0.6680, -0.6692, -0.6711],\n",
      "         [ 8.5918,  8.0394,  3.2524,  ..., -1.2422, -1.2418, -1.2429],\n",
      "         [ 5.2291,  5.9410,  5.4271,  ..., -1.9422, -1.9423, -1.9442]]],\n",
      "       device='mps:0', grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(clean_logits)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclean_cache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/_tensor.py:463\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    460\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    461\u001b[0m     )\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/_tensor_str.py:698\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    697\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/_tensor_str.py:618\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    616\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    617\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 618\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    621\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/_tensor_str.py:350\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    348\u001b[0m     )\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 350\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/_tensor_str.py:138\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_select\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mne\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(clean_logits)\n",
    "\n",
    "print(clean_cache.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4657f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_BASELINE = clean_logit_diff\n",
    "CORRUPTED_BASELINE = corrupted_logit_diff\n",
    "def ioi_metric(logits, answer_token_indices=answer_token_indices):\n",
    "    return (get_logit_diff(logits, answer_token_indices) - CORRUPTED_BASELINE) / (CLEAN_BASELINE  - CORRUPTED_BASELINE)\n",
    "\n",
    "print(f\"Clean Baseline is 1: {ioi_metric(clean_logits).item():.4f}\")\n",
    "print(f\"Corrupted Baseline is 0: {ioi_metric(corrupted_logits).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815909fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens.patching as patching\n",
    "from neel_plotly import line, imshow, scatter\n",
    "\n",
    "resid_pre_act_patch_results = patching.get_act_patch_resid_pre(model, corrupted_tokens, clean_cache, ioi_metric)\n",
    "imshow(resid_pre_act_patch_results, \n",
    "       yaxis=\"Layer\", \n",
    "       xaxis=\"Position\", \n",
    "       x=[f\"{tok} {i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))],\n",
    "       title=\"resid_pre Activation Patching\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
